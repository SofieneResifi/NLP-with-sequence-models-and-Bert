{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8Dpv-Rh9wrm"
   },
   "source": [
    "<h2><center>$${\\color{red}{\\underline{Engineering~Internship:Part~1}}}$$</center></h2>\n",
    "<h3><center>$$\\large{{\\color{green}{Subject:}}~\\underline{Artificial~Emotional~Intelligence}}$$</center></h3>\n",
    "<h3><center>$$This~work~is~done~by:~{\\color{blue}{\\underline{Sofien~Resifi}}}$$</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JakUTt2A_QXY"
   },
   "source": [
    "# $\\large {\\color{red}{\\underline{State~of~the~art}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHnwmcr8DmtP"
   },
   "source": [
    "## ${\\color{green}{I)}}~Overview:$\n",
    "* In This part we are going to see how to deal with our problem step by step.\n",
    "* First we are going to understand the NLP(Natural Language Processing ) world.\n",
    "* We will see how to deal with text data, in other words, we will see how to make a good represntation for text data, also we will see how to process the text data to make good and robust model (here we will see several approachs to deal with NLP).\n",
    "* Once the processing part is done, we are going to move to the modeling part in which we are going to start from the basic model to the complicated model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Crq6reF5Fs_Q"
   },
   "source": [
    "## $\\underline{{\\color{green}{II)}}~What~is~{\\color{blue}{NLP}}({\\color{blue}{N}}atural~{\\color{blue}{L}}anguage~{\\color{blue}{P}}rocessing)?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTjA9DOPF-dn"
   },
   "source": [
    "${\\color{blue}{Natural~language~processing~(NLP):}}$ Natural Language Processing is the technology used to aid computers to understand the human’s natural language.\n",
    "It’s not an easy task teaching machines to understand how we communicate.\n",
    "${\\color{red}{Leand~Romaf}}$, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that “in recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FS_KhuIG-Tm"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"10000\" height=\"500\" src=\"https://enterprisetalk.com/wp-content/uploads/2020/05/IBM-Integrates-Watson-Platform-in-Project-Debater-NLP-Technology.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgOl_uR1HJuM"
   },
   "source": [
    "Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language.\n",
    "The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\n",
    "Most NLP techniques rely on machine learning to derive meaning from human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXPeMT1M7lA8"
   },
   "source": [
    "## $\\underline{{\\color{green}{III)}}~Why~is~NLP~difficult?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be-0OIJE79Gp"
   },
   "source": [
    "Natural Language processing is considered a difficult problem in computer science. It’s the nature of the human language that makes NLP difficult.\n",
    "\n",
    "\n",
    "The rules that dictate the passing of information using natural languages are not easy for computers to understand.\n",
    "\n",
    "\n",
    "Some of these rules can be high-leveled and abstract; for example, when someone uses a sarcastic remark to pass information.\n",
    "\n",
    "\n",
    "On the other hand, some of these rules can be low-levelled; for example, using the character “s” to signify the plurality of items.\n",
    "\n",
    "\n",
    "Comprehensively understanding the human language requires understanding both the words and how the concepts are connected to deliver the intended message.\n",
    "\n",
    "\n",
    "While humans can easily master a language, the ambiguity and imprecise characteristics of the natural languages are what make NLP difficult for machines to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzusJwU679Ei"
   },
   "source": [
    "## $\\underline{{\\color{green}{IV)}}~How~to~deal~with~Natural~Language~Processing~step~by~step?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6csn8TnveKG"
   },
   "source": [
    "### $\\underline{{\\color{red}{1)}}~Text~processing}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qmOgomov3PW"
   },
   "source": [
    "### $~\\underline{{\\color{red}{1}}-{\\color{blue}{1)}}~Text~cleanig}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HJLNt8R6PJt"
   },
   "source": [
    "For the cleaning the text must be representative, in other words we need to keep only the significant words, for example in a text we need to remove all kind of URL, emojis, poncutation, Stop words(like the ,to,and...), Hashtags(in tweets for example). So there is a lot to do with the data, we will see how it will be done in the implemenation part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHm3CVfQwNry"
   },
   "source": [
    "### $\\underline{{\\color{red}{1}}-{\\color{blue}{2)}}~Text~representations}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFO0oJuY79CB"
   },
   "source": [
    "Bold text${\\color{blue}{Syntactic}}$ analysis is the main technique used to complete Natural Language Processing tasks.\n",
    "Here is a description on how they can be used.\n",
    "\n",
    "$\\Longrightarrow$${\\color{red}{Syntax}}$ refers to the arrangement of words in a sentence such that they make grammatical sense.\n",
    "In NLP, syntactic analysis is used to assess how the natural language aligns with the grammatical rules.\n",
    "Computer algorithms are used to apply grammatical rules to a group of words and derive meaning from them.\n",
    "Here are some syntax techniques that can be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wf8ObexdGCes"
   },
   "source": [
    "* $\\underline{{\\color{blue}{Lemmatization~and~Stemming}}}$:\n",
    "\n",
    "   \n",
    "$\\Longrightarrow$ $\\underline{{\\color{green}{\\textbf{Lemmatization}}}}$ entails reducing the various inflected forms of a word into a single form for easy analysis.\n",
    "\n",
    "\n",
    "$\\Longrightarrow$ $\\underline{{\\color{green}{\\textbf{Stemming}}}}$ involves cutting the inflected words to their root form.\n",
    "\n",
    "So we can remarke here that Stemming and Lemmatization are quite the same but with a little bit of difference let's see what's the difference:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"10000\" height=\"500\" src=\"https://miro.medium.com/max/2050/1*ES5bt7IoInIq2YioQp2zcQ.png\">\n",
    "\n",
    "\n",
    "\n",
    "* $\\underline{{\\color{blue}{Word~segmentation:}}}$ It involves dividing a large piece of continuous text into distinct units, in other words , we are going to split a text into a list of words as showing the figure below.\n",
    "<p align=\"center\">\n",
    "  <img width=\"500\" height=\"200\" src=\"https://www.kdnuggets.com/wp-content/uploads/text-tokens-tokenization-manning.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "* $\\underline{{\\color{blue}{Word~Emedding}}}$: Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "$\\underline{\\textbf{What are word embeddings exactly?}}$ Loosely speaking, they are vector representations of a particular word based on a vocablary as showen in the figure bellow.\n",
    "<p align=\"center\">\n",
    "  <img width=\"500\" height=\"200\" src=\"https://resuly.me/img/in_post/2020/embedding/Untitled.png\">\n",
    "\n",
    "So later on we are going to speak more in details about word embedding and representations of word clusters using PCA algorithms which is based on \n",
    "orthogonal projection, so we will see stuffs like the figure bellow:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"400\" src=\"https://amethix.com/wp-content/uploads/2019/05/word_embedding-1.png\">\n",
    "\n",
    "So here we can see similarties between the words, for example ${\\color{red}{cat~and~dog}}$ are close to each other the because they are both animals.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAOLViqj5mXT"
   },
   "source": [
    "## $\\underline{{\\color{green}{V)}}~Modeling}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9d17SaP58Dp"
   },
   "source": [
    "* In this part we are going to see different appproachs that had been used to deal with NLP problems.\n",
    "* So we are going to start from the basic model until we arrive to the newest and most performing algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Te5LK20Q8AO9"
   },
   "source": [
    "### $~\\underline{{\\color{red}{1)}}~Machine~learning~models}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0qyDMAY8MSc"
   },
   "source": [
    "We will create a very basic first model first and then improve it using different other features.\n",
    "\n",
    "Our very first model is a simple ${\\color{red}{TF-IDF}}$ (${\\color{red}{T}}$erm ${\\color{red}{F}}$requency - ${\\color{red}{I}}$nverse ${\\color{red}{D}}$ocument ${\\color{red}{F}}$requency) followed by a simple Logistic Regression. So here let's explain some key words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNAdgaZS7-0q"
   },
   "source": [
    "#### $\\underline{{\\color{red}{1}}-{\\color{blue}{1)}}~What~is~TF-IDF~and~how~does~it~works}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-wZIiHJ-ENz"
   },
   "source": [
    "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP).\n",
    "\n",
    "TF-IDF (term frequency-inverse document frequency) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular.\n",
    "\n",
    "However, if the word Bug appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant. For example, if what we’re doing is trying to find out which topics some NPS responses belong to, the word Bug would probably end up being tied to the topic Reliability, since most responses containing that word would be about that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXAfXgNvAQZ_"
   },
   "source": [
    "$\\underline{{\\color{blue}{\\textbf{How is TF-IDF calculated?}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTLa7eZmAjj2"
   },
   "source": [
    "TF-IDF for a word in a document is calculated by multiplying two different metrics:\n",
    "\n",
    "* The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n",
    "\n",
    "* The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n",
    "So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\n",
    "Multiplying these two numbers results in the TF-IDF score of a word in a document. The higher the score, the more relevant that word is in that particular document.\n",
    "\n",
    "To put it in more formal mathematical terms, the TF-IDF score for the word ${\\color{blue}{t}}$ in the document ${\\color{blue}{d}}$ from the document set ${\\color{blue}{D}}$ is calculated as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FuGtecdZCo0k"
   },
   "source": [
    "<h3><center>$$tfidf(t,d,D)=tf(t,d)*idf(t,D)$$</center></h3>\n",
    "<h2><center>$$where$$</center></h2>\n",
    "<h2><center>$$tf(t,d)=log(1+freq(t,d))$$</center></h2>\n",
    "<h2><center>$$idf(t,D)=log(\\frac{N}{count(d\\in D:t\\in d)})$$</center></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhVcUCiIH5Ln"
   },
   "source": [
    "#### $\\underline{{\\color{red}{1}}-{\\color{blue}{2)}}~Logistic~regression}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCZBGvB-KfbW"
   },
   "source": [
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign. Logistic regression transforms its output using the logistic sigmoid function to return a probability value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CrHZMGY_M8Np"
   },
   "source": [
    "$\\underline{{\\color{blue}{\\textbf{What are the types of logistic regression?}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ImDO-PSNTUd"
   },
   "source": [
    "* ${\\color{red}{1)}}$ Binary (eg. Tumor Malignant or Benign)\n",
    "* ${\\color{red}{2)}}$ Multi-linear functions failsClass (eg. Cats, dogs or Sheep's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TB6elS9sNdKH"
   },
   "source": [
    "Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdPT1bjHNdAT"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"400\" src=\"https://miro.medium.com/proxy/0*gKOV65tvGfY8SMem.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKqPxOYoN1qP"
   },
   "source": [
    "We can call a Logistic Regression a Linear Regression model but the Logistic Regression uses a more complex cost function, this cost function can be defined as the ‘Sigmoid function’ or also known as the ‘logistic function’ instead of a linear function.\n",
    "<h2><center>$$h_{\\theta}(x)=\\frac{1}{1+exp(-\\theta ^Tx)}$$</center></h2>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"400\" src=\"https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w3_logistic_regression_regularization/logistic_regression_simple2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgF7KGuNSFBU"
   },
   "source": [
    "Lets look at the same model with a different data.\n",
    "\n",
    "Instead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f67tcUIqSMY6"
   },
   "source": [
    "#### $\\underline{{\\color{red}{1}}-{\\color{blue}{3)}}What~is~word~counts~and~how~does~it~works?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDo6-flaW7Ct"
   },
   "source": [
    "Word counts is a more simple model then TF-IDF, in word counts the features of the data will be the words the most frequent in the data and for each word correspond a specific count for each document in the corps as shown in the figure bellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFrrdT6bXbZb"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"400\" src=\"https://kavita-ganesan.com/wp-content/uploads/image-5-1024x391.png\">\n",
    "\n",
    "  Next, let's try a very simple model which was quite famous in ancient times - Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdyMnr_jXagb"
   },
   "source": [
    "#### $\\underline{{\\color{red}{1}}-{\\color{blue}{4)}}~Naive~Bayes}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsGKwKhyY4QR"
   },
   "source": [
    "Naive Bayes is a family of algorithms based on applying Bayes theorem with a strong(naive) assumption, that every feature is independent of the others, in order to predict the category of a given sample. They are probabilistic classifiers, therefore will calculate the probability of each category using Bayes theorem, and the category with the highest probability will be output. Naive Bayes classifiers have been successfully applied to many domains, particularly Natural Language Processing(NLP).\n",
    "\n",
    "let's take an example and analyse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pUbGKC3ZVBD"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"400\" src=\"https://miro.medium.com/max/962/0*cKgL5LjvVBeBVIVi.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmlN826GZ9Gz"
   },
   "source": [
    "$\\underline{{\\color{blue}{\\textbf Bayes~theorem}}}$\n",
    "<p align=\"center\">\n",
    "  <img width=\"400\" height=\"200\" src=\"https://cdn1.byjus.com/wp-content/uploads/2018/11/maths/2016/03/03054449/Conditional-Probability.jpg\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gT28qV6Za5rd"
   },
   "source": [
    "In our case, the probability that we wish to calculate can be calculated as:\n",
    "\n",
    "<h3><center>$$P(Sport/very~close~game)=\\frac{P(very~close~game/Sport)P(Sport)}{P(very~close~game)}$$</center></h3>\n",
    "Because we are only trying to find out which category (Sports or Not Sports) has a higher probability, it makes sense to discard the divisor P(a very close game), and compare only:\n",
    "<h2><center>$$P(very~close~game/Sport)P(Sport)$$</center></h2>\n",
    "<h2><center>$$with$$</center></h2>\n",
    "<h2><center>$$P(very~close~game/Not~Sport)P(Not~Sport)$$</center></h2>\n",
    "\n",
    "The Naive Bayes model is a Naive model because of its assumptions:like \"$\\underline{{\\color{red}{All~the~features~are~independent}}}$\" which is wrong in the reality the dependencies of features is very important to build a robust model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjHFFLryefTD"
   },
   "source": [
    "So far we have seen some Machine Learning algrothims that can help us to deal with NLP problems, but their are others algorithms like desicion tree algoritms. \n",
    "Our goal here is to create a good and robust Machine Learning model, that's why we can use one of the best techniques which is Ensembling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sT95OkDwfBcY"
   },
   "source": [
    "#### $\\underline{{\\color{red}{1}}-{\\color{blue}{5)}}~Ensembling}$ 😎😎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RWd2y3Vgh-r"
   },
   "source": [
    "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.We will speak more in details about Ensembling in the implementation part.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"1000\" height=\"500\" src=\"https://miro.medium.com/max/2556/1*P0ns6A56MtpGFMQ2g47IYA.png\">\n",
    "\n",
    "  We will speak more in details about Ensembling in the implementation part.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJ1n2FHRdzBZ"
   },
   "source": [
    "#### $\\underline{{\\color{red}{1}}-{\\color{blue}{6)}}~Conclusion}$ 👏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnw_2g-jd6Ig"
   },
   "source": [
    "So to sum up,Machine Leanring is one of the method that can help us to deal with NLP Problems. But Machine Learning isn't the best method, we have a more powerful and performing method which is Deep Learning. Let's see how does it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqxxIpsNic3H"
   },
   "source": [
    "### $\\underline{{\\color{red}{2)}}~Deep~Learning~models}$🤯🤯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xswjn0npi-KK"
   },
   "source": [
    "#### $\\underline{{\\color{red}{2}}-{\\color{blue}{1)}}~What~is~Deep~Learning?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mhbl_5siM6rH"
   },
   "source": [
    "Deep learning is a type of machine learning (ML) and artificial intelligence (AI) that imitates the way humans brain certain types of knowledge. Deep learning is an important element of data science, which includes statistics and predictive modeling. It is extremely beneficial to data scientists who are tasked with collecting, analyzing and interpreting large amounts of data; deep learning makes this process faster and easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8Yi87jKMHy5"
   },
   "source": [
    "#### $\\underline{{\\color{red}{2}}-{\\color{blue}{2)}}~What~are~Deep~Learning~neural~networks?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9xWy8HmNd8z"
   },
   "source": [
    "A type of advanced machine learning algorithm, known as artificial neural networks, underpins most deep learning models. As a result, deep learning may sometimes be referred to as deep neural learning or deep neural networking.\n",
    "\n",
    "Neural networks come in several different forms, including ${\\color{blue}{recurrent~neural~networks}}$, ${\\color{blue}{convolutional~neural~networks}}$... and each has benefits for specific use cases. However, they all function in alomost a similar ways, by feeding data in and letting the model figure out for itself whether it has made the right interpretation or decision about a given data element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgKo1bgfOwQS"
   },
   "source": [
    "#### $\\underline{{\\color{red}{2}}-{\\color{blue}{3)}}~Simple~neural~network}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5Rl5h1fPXdb"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"1000\" height=\"500\" src=\"https://pyimagesearch.com/wp-content/uploads/2016/08/simple_neural_network_header.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li9290E-TVuk"
   },
   "source": [
    "The fact of moving from the input layer to the output layer is called forward propagation. Once the forward probagation is done, we need to update our parameters to minimize the cost function let's see how the forward propagation and backward propagation works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yUUXwJTUcel"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"400\" src=\"https://www.siddharthasahai.com/images/notes/neuralnetworksdeeplearning/codeeg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksz-eHDvP8he"
   },
   "source": [
    "This is a three layers neural network( we don't count the input layer), so this kind of architecture is the basics of deep learning. So let's move on to a more complicated architecture where we are going to involve convolutions and maxpooling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBAy6shSU4Sm"
   },
   "source": [
    "#### $\\underline{{\\color{red}{2}}-{\\color{blue}{4)}}~Convolutional~neural~network}bold text$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi8SpqPVV48t"
   },
   "source": [
    "${\\color{blue}{\\textbf{What is convolution}}}$\n",
    "\n",
    "In neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. Objects detections, recognition faces and also we can use it for NLP tasks(text classifcation), are some of the areas where CNNs are widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urJKDt6wWPKH"
   },
   "source": [
    "${\\color{green}{Convolution}}$ are meant to extract features from the inputs, and to do so we use ${\\color{red}{filters}}$, each filter have a specific function, in other words, the feature extracted depends on the filter used, and in order to emphasize those features we use what we call ${\\color{red}{maxpooling}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnbWokVjYLPB"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"1000\" src=\"http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-12.05.40-PM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "may6vzzJYU0j"
   },
   "source": [
    "$\\underline{{\\color{blue}{\\textbf{Disadvantage of simple neural network and convolutional neural network in NLP context}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0E5rHL4Ykhb"
   },
   "source": [
    "The convolution neural network is a very good model for images, but it isn't the case for NLP,the above architectures have a big disadvantage with NLP tasks, pixels close to each others are more likely to be semantically related (part of the same object), but the same isn’t always true for words. In many languages, parts of phrases could be separated by several other words, in other words, the convolution neural network don't take into account the postionning of the words and the dependencies between them which is a very important thing to consider in NLP. So let's see an example to understand more this disadvatage.\n",
    "\n",
    "$First~example$\n",
    "<h2><center>$$My~{\\color{red}{\\underline{car}}},~which~I~bought~last~year~with....,~{\\color{red}{\\underline{was}}}~very~expensive$$</center></h2>\n",
    "\n",
    "$Second~example:~In~english~the~adjectve~comes~befor~the~noun$\n",
    "<h2><center>$$A~{\\color{red}{\\underline{beautiful}}}~{\\color{blue\n",
    "}{\\underline{car}}~Not~A~{\\color{blue}{\\underline{car}}}~{\\color{red}{\\underline{beautiful}}}}$$</center></h2>\n",
    "\n",
    "The convolutional neurla network doesn't distinguish between the positionning of the adjetive and the noun and it doesn't take into account the dependencies between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uTHyjgZduOD"
   },
   "source": [
    "It seems like CNNs wouldn’t be a good fit for NLP tasks😢😢😢. ${\\color{green}{Recurrent~Neural~Networks}}$ make more intuitive sense. They resemble how we process language (or at least how we think we process language): Reading sequentially from left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVsytHhtiG-7"
   },
   "source": [
    "#### $\\underline{{\\color{red}{2}}-{\\color{blue}{5)}}~Sequence~models~{\\color{red}{\\&}}~Recurrent~Neural~Networks}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXQU9br1B4ak"
   },
   "source": [
    "$\\underline{{\\color{blue}{Sequence~Modeling:}}}$\n",
    "\n",
    "Sequence Modeling is the task of predicting what word/letter comes next. Unlike CNN, in sequence modeling, the current output is dependent on the previous input and the length of the input is not fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onuU6TZWHWnR"
   },
   "source": [
    "$\\underline{{\\color{blue}{Recurrent~Neural~Networks(RNN):}}}$ are a type of Neural Network where the output from the previous step is fed as input to the current step.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=1000\" height=\"300\" src=\"https://miro.medium.com/max/1050/1*I2TsC5UDAPykJni772v9cQ.png\">\n",
    "\n",
    "In RNN, you can see that the output of the first time step is fed as input along with the original input to the next time step.\n",
    "The input to the function is denoted in orange color and represented as an xᵢ. The weights associated with the input is denoted using a vector U and the hidden representation (sᵢ) of the word is computed as a function of the output of the previous time step and current input along with bias. The output of the hidden represented (sᵢ) is given by the following equation:\n",
    "\n",
    "<h2><center>$$s_i=g(Ux_i+Ws_{i-1}+b)$$</center></h2>\n",
    "<h2><center>$$with~g~the~activation~function~inside~the~recurrent~network$$</center></h2>\n",
    "<h2><center>$$ \\hat{y}_i=Softmax(Vs_i+c)$$</center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzdEM7c9K-e1"
   },
   "source": [
    "So here we saw the intuition behind the RNN, so now let's see the most known derived architectures from RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gKOISE9LTED"
   },
   "source": [
    "$\\underline{\\Large{\\color{red}{Long~Short~Term~Memory~networks}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvKPIY-aNbrc"
   },
   "source": [
    "${\\color{blue}{L}}$ong ${\\color{blue}{S}}$hort ${\\color{blue}{T}}$erm ${\\color{blue}{M}}$emory Networks – usually just called ${\\color{blue}{LSTM}}$ – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by ${\\color{green}{Hochreiter~\\&~Schmidhuber}}$ (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "\n",
    "All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"300\" src=\"https://i.pinimg.com/originals/27/95/bc/2795bc16b012322f7767cd4d940ba2e3.png\">\n",
    "\n",
    "LSTM will take into account the positionning of words as shown in the exemple bellow:\n",
    "In english the adjectve comes befor the noun\n",
    "\n",
    "<h2><center>$$A~{\\color{red}{\\underline{beautiful}}}~{\\color{blue\n",
    "}{\\underline{car}}~Not~A~{\\color{blue}{\\underline{car}}}~{\\color{red}{\\underline{beautiful}}}}$$</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSxiugCu2VRR"
   },
   "source": [
    "$\\underline{\\Large{\\color{red}{Bidirectional~Long~Short~Term~Memory~networks}}}$\n",
    "\n",
    "Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together. This structure allows the networks to have both backward and forward information about the sequence at every time step\n",
    "\n",
    "Using bidirectional will run your inputs in two ways, one from past to future and one from future to past and what differs this approach from unidirectional is that in the LSTM that runs backwards you preserve information from the future and using the two hidden states combined you are able in any point in time to preserve information from both past and future.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"300\" src=\"https://miro.medium.com/max/1000/1*B5NHtY8_Y4we0DE4Y-acBA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9gEJq8G2x1J"
   },
   "source": [
    "$\\underline{\\Large{\\color{red}{Gated~recurrent~unit}}}$\n",
    "\n",
    " GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results.\n",
    "\n",
    " As mentioned above, GRUs are improved version of standard recurrent neural network. But what makes them so special and effective?\n",
    "To solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"800\" height=\"300\" src=\"https://miro.medium.com/max/1050/1*7oE-4Wg6bZ7u8yDf5cjJPA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fl7hddE44Jj"
   },
   "source": [
    "### $\\underline{{\\color{red}{3)}}~Attention~\\&~transformers~{\\color{red}{\\Longrightarrow}}~BERT}$🤯🤯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqgHezVJ5q6H"
   },
   "source": [
    "#### $\\underline{{\\color{red}{3}}-{\\color{blue}{1)}}~Attention:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56l_hMWyprf8"
   },
   "source": [
    "This is the toughest and most tricky part. If we understand the intiuition and working of attention block , understanding transformers and transformer based architectures like BERT will be a piece of cake. This is the part where I spent the most time.\n",
    "\n",
    "${\\color{red}{Attention}}$ is arguably one of the most powerful concepts in the deep learning field nowadays. It is based on a common-sensical intuition that we “attend to” a certain part when processing a large amount of information.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"500\" height=\"300\" src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0306457318305612-gr4.jpg\">\n",
    "\n",
    "With the Attention mechanism we will give more importance to a part of a sentence or a word and that's what the human does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k355v-Xb5_Pv"
   },
   "source": [
    "#### $\\underline{{\\color{red}{3}}-{\\color{blue}{2)}}~Transformers:}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4NV_oFm3AAP"
   },
   "source": [
    "The transformers are mainly formed by two blocks:the encoder and the decoder.\n",
    "\n",
    "$\\underline{{\\color{blue}{Encoder}}}$\n",
    "\n",
    "The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"300\" src=\"https://miro.medium.com/max/1584/0*ifFEkhW2QOmK2vqN.png\">\n",
    "\n",
    " The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.\n",
    "\n",
    "The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position. \n",
    "\n",
    "$\\underline{{\\color{blue}{Decoder}}}$\n",
    "\n",
    "The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"600\" height=\"300\" src=\"https://i.imgur.com/mTJRnlO.png\n",
    "  \">\n",
    "\n",
    "  So here the final structure of a transfomer as shown the figure bellow:\n",
    "\n",
    "\n",
    "  <p align=\"center\">\n",
    "  <img width=\"600\" height=\"600\" src=\"https://miro.medium.com/max/1284/1*1BFAQXkNiLySIhB__24EkQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cYtlmQO6MM0"
   },
   "source": [
    "#### $\\underline{{\\color{red}{3}}-{\\color{blue}{2)}}~{\\color{green}{B}}idirectional~{\\color{green}{E}}ncoder~{\\color{green}{R}}epresentations~{\\color{green}{f}}rom~{\\color{green}{T}}ransformers({\\color{green}{BERT}}):}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-y_KG3k0oJVL"
   },
   "source": [
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection. (In NLP, this process is called attention.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bw5JrqBUybB-"
   },
   "source": [
    "$\\underline{{\\color{blue}{\\textbf{How BERT works}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o184pueZ1HYV"
   },
   "source": [
    "The goal of any given NLP technique is to understand human language as it is spoken naturally. In BERT's case, this typically means predicting a word in a blank. To do this, models typically need to train using a large repository of specialized, labeled training data. This necessitates laborious manual data labeling by teams of linguists.\n",
    "\n",
    "BERT, however, was pre-trained using only an unlabeled, plain text corpus (namely the entirety of the English Wikipedia, and the Brown Corpus). It continues to learn unsupervised from the unlabeled text and improve even as its being used in practical applications (ie Google search). Its pre-training serves as a base layer of \"knowledge\" to build from. From there, BERT can adapt to the ever-growing body of searchable content and queries and be fine-tuned to a user's specifications. This process is known as ${\\color{red}{transfer~ learning}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0A-8PTxk76Mv"
   },
   "source": [
    "$\\underline{{\\color{blue}{\\textbf{What is BERT used for?}}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVZZWs518EtJ"
   },
   "source": [
    "BERT is currently being used at Google to optimize the interpretation of user search queries. BERT excels at several functions that make this possible, including:\n",
    "\n",
    "* Sequence-to-sequence based language generation tasks such as:\n",
    "\n",
    "$\\Longrightarrow$ Question answering\n",
    "\n",
    "$\\Longrightarrow$ Abstract summarization\n",
    "\n",
    "$\\Longrightarrow$ Sentence prediction\n",
    "\n",
    "$\\Longrightarrow$ Conversational response generation\n",
    "\n",
    "* Natural language understanding tasks such as:\n",
    "\n",
    "$\\Longrightarrow$ Polysemy and Coreference (words that sound or look the same but have different meanings) resolution\n",
    "\n",
    "$\\Longrightarrow$ Word sense disambiguation\n",
    "\n",
    "$\\Longrightarrow$ Natural language inference\n",
    "\n",
    "$\\Longrightarrow$ ${\\color{green}{\\textbf{Sentiment classification}}}$\n",
    "\n",
    " <p align=\"center\">\n",
    "  <img width=\"600\" height=\"600\" src=\"https://miro.medium.com/max/1766/0*C6-mMLW1vrZvb90H\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUCU_2KxAtaE"
   },
   "source": [
    "## $\\underline{{\\color{green}{VI)}}~Conclusion}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqdtJaPiA7Hx"
   },
   "source": [
    "To sum up, in this notebook we have seen the deepest secrets of the NLP world,we have seen how to deal with the sentiement analysis classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cd_KPWAVIV01"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Internship_Biware_Consulting.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
